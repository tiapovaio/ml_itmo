# Генерация текста с использованием LSTM и марковских цепей

В этом проекте исследуется задача генерации текста на основе обучающихся моделей последовательностей: LSTM и марковской цепи. Работа включает предварительную обработку текста, токенизацию с использованием BPE, обучение нейросети и генерацию с помощью двух стратегий (жадная и вероятностная).

---

## Описание задания

1. Выбран набор текстов (например, книга, новости, статьи)
2. Последовательности разбиты на предложения или логические фрагменты
3. Применена **Byte-Pair Encoding (BPE)** токенизация 
   - Учтены токены конца предложения
4. Создан датасет для **предсказания следующего токена по текущему префиксу**
5. Построена модель на **базе LSTM**:
   - Эмбеддинги с использованием `nn.Embedding`
   - Несколько LSTM-слоёв с `nn.LSTM`
   - Предсказание вероятности следующего токена
6. Модель обучена с помощью PyTorch
7. Реализована генерация текста:
   - **Жадная стратегия** (argmax)
   - **Стохастическая выборка** по предсказанным вероятностям
8. Те же пункты реализованы с помощью **марковской цепи**
